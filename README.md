
# Recreates SpliceBERT Results and Runs Small Scale SimCSP

## TODO: 
- rebuild pre-train to handle mulitple loss functions
- reproduce NMI results on labelled movie reviews dataset 
- get SCCS and NMI SpliceBERT 

## EXPERIMENTS: 
- 1: relationship between f1/sccs in pretrain and fine-tune and its affects on F1 score 
    - uses the last layer's [CLS] token 
    - uses SpliceBERT-human, evaluated using f1/sccs on spliceator and NMI on hg19
    - could be improved by using SpliceBERT and NMI on spliceator 
- 2 - hyperparameter search over pre-train w/ SCCS validation
- 3: experiment with chopping off 1 layers transformer and doing CL + fine-tune on that
- 4: experiment with chopping off 2 transformer layers and doing CL + fine-tune on that
- 5: freeze the transformer layers and only fine-tune the classifier 
    - this will tell you if the internal representation is being improved 

## METRICS:
- SCCS : spearman correlation w/ cosine similarity 
- NMI : normalized mutual information 
- F1 : linear combination of precision and recall

## COMPUTE QUEUE:

## NOTES: 
- cite UMAP and leiden algorithm + give more explanation on generation
- hrg uses hg38 and umap plotting uses hg19 but K562 uses hg38 so can only be used w/ msg
    - if pre-training on msg then should use spliceator test set for NMI evaluation
- spliceator num_samples 44,152
- until SpliceBERT-human MLM is implemented correctly, might be learning from hrg during CL 
- view log files generated by tf: 
    - download log.tf file to runs on mac 
    - use bash run.sh to initialize docker container from terminal
    - use terminal inside of docker desktop to run following command : tensorboard --logdir=runs --host 0.0.0.0
    - use link in chrome browser and adjust  2nd number to 6.0.0.6 to view logs

## EXPERIMENTAL PIPELINE:
- pretrain.sh
- finetune.sh

## KNOWN IMPROVEMENTS: 
- bigger model
- more diverse training data
- use middle transformer layers 
- use mean pooling across some linear combination of hidden layers 

## STORY
__(character)__ needed _____ because _____ but _____ so _____  finally ____

## IDEAS: 
- train models with frozen classifier layers to evaluate the quality of the embeddings (inspect quality of pre-trained embeddings)
- use a MLM and CL loss function together during pre-train (multiple loss functions)
- use a splice site location tool to generate a better MLM training set w/ sup. simCSP (synthetic data and sup. CL)
- look for papers that used CL to improve performance on the SUBJ dataset (b/c it is a binary classification task)

